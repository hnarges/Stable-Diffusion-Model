# Stable-Diffusion-Model
Stable Diffusion model is used for generating images based on text prompts.
# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fgJMDrP5ThZR6eorpJ03eI8zJa0ycXO
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torchvision.utils import save_image
import numpy as np
import matplotlib.pyplot as plt

latent_dims = 2  # For 2D visualization
num_epochs = 30
batch_size = 128
capacity = 64  # Capacity for model's complexity
learning_rate = 1e-3
variational_beta = 1  # Beta term for KL-divergence
use_gpu = torch.cuda.is_available()

import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST

# Image transformation pipeline
img_transform = transforms.Compose([
    transforms.ToTensor()
])

# Download and prepare the MNIST training dataset
train_dataset = MNIST(root='./data', train=True, download=True, transform=img_transform)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Download and prepare the MNIST test dataset
test_dataset = MNIST(root='./data', train=False, download=True, transform=img_transform)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

import torch
import torch.nn as nn
import torch.nn.functional as F

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        c = capacity  # Using the defined capacity

        # Convolutional layers
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1)  # Output: c x 14 x 14
        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)  # Output: 2c x 7 x 7

        # Fully connected layers for the latent variables (mu and logvar)
        self.fc_mu = nn.Linear(c*2*7*7, latent_dims)       # Output: latent_dims for mu
        self.fc_logvar = nn.Linear(c*2*7*7, latent_dims)   # Output: latent_dims for logvar

    def forward(self, x):
        # Apply the convolutional layers with ReLU activation
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))

        # Flatten the output to feed into fully connected layers
        x = x.view(x.size(0), -1)  # Flatten the tensor

        # Get the mean (mu) and log variance (logvar) from two separate fully connected layers
        x_mu = self.fc_mu(x)
        x_logvar = self.fc_logvar(x)

        return x_mu, x_logvar

import torch
import torch.nn as nn
import torch.nn.functional as F

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        c = capacity  # Using the defined capacity

        # Fully connected layer to expand the latent vector
        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)

        # Transpose convolutional layers (reverse of the Encoder)
        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)  # Output: c x 14 x 14
        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)  # Output: 1 x 28 x 28

    def forward(self, x):
        # Fully connected layer to expand the latent vector back to a feature map
        x = self.fc(x)
        x = x.view(x.size(0), capacity*2, 7, 7)  # Unflatten to feature map of size (batch_size, capacity*2, 7, 7)

        # Transpose convolution layers to upsample the feature maps
        x = F.relu(self.conv2(x))  # Output: c x 14 x 14
        x = self.conv1(x)  # Output: 1 x 28 x 28

        # Apply sigmoid activation to the final output
        x = torch.sigmoid(x)

        return x

import torch
import torch.nn as nn

class VariationalAutoencoder(nn.Module):
    def __init__(self):
        super(VariationalAutoencoder, self).__init__()

        # Define the Encoder and Decoder instances
        self.encoder = Encoder()  # Instantiate the Encoder
        self.decoder = Decoder()  # Instantiate the Decoder

    def forward(self, x):
        # Pass the input through the encoder to get the mean and log variance of the latent space
        latent_mu, latent_logvar = self.encoder(x)

        # Sample from the latent space using the reparameterization trick
        latent = self.latent_sample(latent_mu, latent_logvar)

        # Pass the sampled latent vector through the decoder to reconstruct the image
        x_recon = self.decoder(latent)

        return x_recon, latent_mu, latent_logvar

    def latent_sample(self, mu, logvar):
        if self.training:
            # Reparameterization trick: sample from a normal distribution
            std = logvar.mul(0.5).exp_()  # Calculate standard deviation from log variance
            eps = torch.empty_like(std).normal_()  # Sample epsilon from a standard normal distribution
            return eps.mul(std).add_(mu)  # Reparameterized sample: z = mu + sigma * epsilon
        else:
            # During inference, just use the mean (mu) as the latent representation
            return mu

import torch
import torch.nn.functional as F

def vae_loss(recon_x, x, mu, logvar):
    # Reconstruction loss: Binary Cross-Entropy
    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')

    # KL Divergence
    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Total loss: Reconstruction loss + Variational Beta * KL Divergence
    return recon_loss + variational_beta * kldivergence

import torch.optim as optim

# Initialize VAE
vae = VariationalAutoencoder()
device = torch.device("cuda:0" if use_gpu and torch.cuda.is_available() else "cpu")
vae = vae.to(device)

# Optimizer
optimizer = optim.Adam(vae.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_idx, (data, _) in enumerate(train_dataloader):
        data = data.to(device)

        optimizer.zero_grad()
        recon_batch, mu, logvar = vae(data)
        loss = vae_loss(recon_batch, data, mu, logvar)
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_dataloader.dataset)}] Loss: {loss.item()}')

num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)
print(f'Number of parameters: {num_params}')

optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)

# Set to training mode
vae.train()

train_loss_avg = []

print('Training ...')
for epoch in range(num_epochs):
    train_loss_avg.append(0)
    num_batches = 0

    for image_batch, _ in train_dataloader:
        # Move batch to the same device as the model
        image_batch = image_batch.to(device)

        # Zero gradients before the backward pass
        optimizer.zero_grad()

        # Forward pass: Get the reconstruction, mu, and logvar
        recon_batch, mu, logvar = vae(image_batch)

        # Compute the VAE loss
        loss = vae_loss(recon_batch, image_batch, mu, logvar)

        # Backward pass: Compute gradients
        loss.backward()

        # Update model parameters
        optimizer.step()

        # Accumulate loss for averaging
        train_loss_avg[-1] += loss.item()
        num_batches += 1

    # Compute average loss for the epoch
    train_loss_avg[-1] /= num_batches

    # Print the average loss for the epoch
    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))

import matplotlib.pyplot as plt
plt.ion()

fig = plt.figure()
plt.plot(train_loss_avg)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

# set to evaluation mode
vae.eval()

test_loss_avg, num_batches = 0, 0
for image_batch, _ in test_dataloader:

    with torch.no_grad():

        image_batch = image_batch.to(device)

        # vae reconstruction
        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)

        # reconstruction error
        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)

        test_loss_avg += loss.item()
        num_batches += 1

test_loss_avg /= num_batches
print('average reconstruction error: %f' % (test_loss_avg))

import numpy as np
import matplotlib.pyplot as plt
plt.ion()

import torchvision.utils

vae.eval()

# This function takes as an input the images to reconstruct
# and the name of the model with which the reconstructions
# are performed
def to_img(x):
    x = x.clamp(0, 1)
    return x

def show_image(img):
    img = to_img(img)
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

def visualise_output(images, model):

    with torch.no_grad():

        images = images.to(device)
        images, _, _ = model(images)
        images = images.cpu()
        images = to_img(images)
        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()
        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))
        plt.show()

images, labels = next(iter(test_dataloader))

# First visualise the original images
print('Original images')
show_image(torchvision.utils.make_grid(images[1:50],10,5))
plt.show()

# Reconstruct and visualise the images using the vae
print('VAE reconstruction:')
visualise_output(images, vae)

def interpolation(lambda1, model, img1, img2):
    with torch.no_grad():
        # Move images to the same device as the model
        img1 = img1.to(device)
        img2 = img2.to(device)

        # Encode both images to get their latent means (mu)
        mu1, _ = model.encoder(img1)
        mu2, _ = model.encoder(img2)

        # Interpolate between the two latent vectors
        inter_latent = lambda1 * mu1 + (1 - lambda1) * mu2

        # Decode the interpolated latent vector to reconstruct the image
        inter_image = model.decoder(inter_latent)
        inter_image = inter_image.cpu()

        return inter_image

vae.eval()

# Sort part of the test set by digit
digits = [[] for _ in range(10)]
for img_batch, label_batch in test_dataloader:
    for i in range(img_batch.size(0)):
        digits[label_batch[i]].append(img_batch[i:i+1])
    if sum(len(d) for d in digits) >= 1000:
        break

# Interpolation lambdas
lambda_range = np.linspace(0, 1, 10)

fig, axs = plt.subplots(2, 5, figsize=(15, 6))
fig.subplots_adjust(hspace=0.5, wspace=0.001)
axs = axs.ravel()

for ind, l in enumerate(lambda_range):
    inter_image = interpolation(float(l), vae, digits[7][0], digits[1][0])
    inter_image = to_img(inter_image)
    image = inter_image.numpy()
    axs[ind].imshow(image[0, 0, :, :], cmap='gray')
    axs[ind].set_title('lambda_val=' + str(round(l, 1)))

plt.show()

# load a network that was trained with a 2d latent space
if latent_dims != 2:
    print('Please change the parameters to two latent dimensions.')

with torch.no_grad():

    # create a sample grid in 2d latent space
    latent_x = np.linspace(-1.5,1.5,20)
    latent_y = np.linspace(-1.5,1.5,20)
    latents = torch.FloatTensor(len(latent_y), len(latent_x), 2)
    for i, lx in enumerate(latent_x):
        for j, ly in enumerate(latent_y):
            latents[j, i, 0] = lx
            latents[j, i, 1] = ly
    latents = latents.view(-1, 2) # flatten grid into a batch

    # reconstruct images from the latent vectors
    latents = latents.to(device)
    image_recon = vae.decoder(latents)
    image_recon = image_recon.cpu()

    fig, ax = plt.subplots(figsize=(10, 10))
    show_image(torchvision.utils.make_grid(image_recon.data[:400],20,5))
    plt.show()

!pip install --upgrade transformers==4.40.0 diffusers==0.27.2 ftfy==6.2.0 accelerate datasets bitsandbytes

from argparse import Namespace
import math
from tqdm.auto import tqdm
from PIL import Image
import torch
from torch import autocast
from torchvision import transforms
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler, DDPMScheduler
from accelerate import Accelerator
from accelerate.utils import set_seed
import bitsandbytes as bnb

height = 512                        # default height of Stable Diffusion
width = 512                         # default width of Stable Diffusion
num_inference_steps = 50           # Number of denoising steps
guidance_scale = 8                # Scale for classifier-free guidance
generator = torch.manual_seed(64)   # Seed generator to create the inital latent noise
batch_size = 1
torch_device = "cuda" if torch.cuda.is_available() else "cpu"
device = torch_device

# Load the autoencoder model which will be used to decode the latents into image space.
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
# The UNet model for generating the latents
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet")
scheduler = PNDMScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")
vae = vae.to(torch_device)
text_encoder = text_encoder.to(torch_device)
unet = unet.to(torch_device)

def latents_to_pil(latents):

    # bath of latents -> list of images
    latents = (1 / 0.18215) * latents
    with torch.no_grad():
        image = vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
    images = (image * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]
    return pil_images

def prompt_to_emb(prompt):
    # Tokenize the prompts
    batch_size = len(prompt)

    # Convert the text prompts into tokenized inputs with fixed max length
    text_inputs = tokenizer(
        prompt,
        padding="max_length",
        max_length=77,  # Maximum token length
        truncation=True,  # Truncate if the prompt exceeds max_length
        return_tensors="pt",  # Return as PyTorch tensors
    )
    text_input_ids = text_inputs.input_ids

    # Encode the tokenized inputs into embeddings using the text encoder
    prompt_embeds = text_encoder(text_input_ids.to(device))
    prompt_embeds = prompt_embeds.last_hidden_state  # Extract the embeddings

    # Get the data type of the embeddings
    prompt_embeds_dtype = prompt_embeds.dtype

    # Ensure embeddings are in the correct dtype and on the correct device
    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)

    # Get the sequence length from the embeddings
    _, seq_len, _ = prompt_embeds.shape

    # Repeat and reshape the embeddings for further processing
    prompt_embeds = prompt_embeds.repeat(1, 1, 1)
    prompt_embeds = prompt_embeds.view(batch_size * 1, seq_len, -1)

    # Create negative prompt embeddings
    # Here we use an empty string for the negative prompt
    negative_prompt = [""] * batch_size
    negative_text_inputs = tokenizer(
        negative_prompt,
        padding="max_length",
        max_length=77,
        truncation=True,
        return_tensors="pt",
    )
    negative_text_input_ids = negative_text_inputs.input_ids

    negative_prompt_embeds = text_encoder(negative_text_input_ids.to(device))
    negative_prompt_embeds = negative_prompt_embeds.last_hidden_state

    # Ensure negative embeddings are in the correct dtype and on the correct device
    negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)

    # Repeat and reshape the negative embeddings
    negative_prompt_embeds = negative_prompt_embeds.repeat(1, 1, 1)
    negative_prompt_embeds = negative_prompt_embeds.view(batch_size * 1, seq_len, -1)

    # Concatenate the unconditional and prompt embeddings
    concatenated_embeddings = torch.cat([negative_prompt_embeds, prompt_embeds])

    # Return the concatenated embeddings
    return concatenated_embeddings

def emb_to_latents(text_embeddings):

    scheduler.set_timesteps(num_inference_steps)

    # Generate random latent noise for the input to the model
    latents = torch.randn((1, 4, 64, 64),dtype=torch.float32).to(torch_device)

    for t in tqdm(scheduler.timesteps):
        # Duplicate the random latent noise
        latent_model_input = torch.cat([latents] * 2)

        # Scale the model input
        latent_model_input = scheduler.scale_model_input(latent_model_input, t)
        with torch.no_grad():
            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings,return_dict=False,added_cond_kwargs={'text_embeds':text_embeddings})[0]

        # Apply classifier-free guidance scale
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        latents = scheduler.step(noise_pred, t, latents,return_dict=False)[0]

    return latents

prompt = ["a photograph of an astronaut riding a horse"]
text_embeddings = prompt_to_emb(prompt)
latents = emb_to_latents(text_embeddings)
image = latents_to_pil(latents)
image[0]

prompt = ["a campfire (oil on canvas)"]
text_embeddings = prompt_to_emb(prompt)
latents = emb_to_latents(text_embeddings)
image = latents_to_pil(latents)
image[0]

